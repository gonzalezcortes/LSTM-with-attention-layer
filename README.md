# LSTM with Explainability using an attention layer

In this notebook introduces a prediction using a Long Short-Term Memory (LSTM) with explainability. The data set used is "Power consumption of Tetouan city Data Set" from UCI Machine Learning Repository.

Also a prediction using XGBoost with different explanatory framework named SHAP was used to compare the LSTM with explainability.

![alt text](https://github.com/gonzalezcortes/LSTM-with-attention-layer/blob/main/attention_values.jpg?raw=true)

Keras architecture

![alt text](https://github.com/gonzalezcortes/LSTM-with-attention-layer/blob/main/index.png?raw=true)
